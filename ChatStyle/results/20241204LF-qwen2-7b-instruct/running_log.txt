[WARNING|2024-12-04 07:50:16] logging.py:162 >> We recommend enable mixed precision training.

[WARNING|2024-12-04 07:50:16] logging.py:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.

[INFO|2024-12-04 07:50:16] parser.py:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: None

[INFO|2024-12-04 07:50:16] configuration_utils.py:677 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/config.json

[INFO|2024-12-04 07:50:16] configuration_utils.py:746 >> Model config Qwen2Config {
  "_name_or_path": "/root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


[INFO|2024-12-04 07:50:16] tokenization_utils_base.py:2209 >> loading file vocab.json

[INFO|2024-12-04 07:50:16] tokenization_utils_base.py:2209 >> loading file merges.txt

[INFO|2024-12-04 07:50:16] tokenization_utils_base.py:2209 >> loading file tokenizer.json

[INFO|2024-12-04 07:50:16] tokenization_utils_base.py:2209 >> loading file added_tokens.json

[INFO|2024-12-04 07:50:16] tokenization_utils_base.py:2209 >> loading file special_tokens_map.json

[INFO|2024-12-04 07:50:16] tokenization_utils_base.py:2209 >> loading file tokenizer_config.json

[INFO|2024-12-04 07:50:17] parser.py:355 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: None

[INFO|2024-12-04 07:50:17] tokenization_utils_base.py:2475 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|2024-12-04 07:50:17] configuration_utils.py:677 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/config.json

[INFO|2024-12-04 07:50:17] configuration_utils.py:746 >> Model config Qwen2Config {
  "_name_or_path": "/root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


[INFO|2024-12-04 07:50:17] tokenization_utils_base.py:2209 >> loading file vocab.json

[INFO|2024-12-04 07:50:17] tokenization_utils_base.py:2209 >> loading file merges.txt

[INFO|2024-12-04 07:50:17] tokenization_utils_base.py:2209 >> loading file tokenizer.json

[INFO|2024-12-04 07:50:17] tokenization_utils_base.py:2209 >> loading file added_tokens.json

[INFO|2024-12-04 07:50:17] tokenization_utils_base.py:2209 >> loading file special_tokens_map.json

[INFO|2024-12-04 07:50:17] tokenization_utils_base.py:2209 >> loading file tokenizer_config.json

[INFO|2024-12-04 07:50:17] tokenization_utils_base.py:2475 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|2024-12-04 07:50:17] logging.py:157 >> Replace eos token: <|im_end|>

[INFO|2024-12-04 07:50:17] logging.py:157 >> Loading dataset alpaca_sft_data.json...

[INFO|2024-12-04 07:50:20] configuration_utils.py:677 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/config.json

[INFO|2024-12-04 07:50:20] configuration_utils.py:746 >> Model config Qwen2Config {
  "_name_or_path": "/root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


[INFO|2024-12-04 07:50:20] modeling_utils.py:3934 >> loading weights file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/model.safetensors.index.json

[INFO|2024-12-04 07:50:20] modeling_utils.py:1670 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.

[INFO|2024-12-04 07:50:20] configuration_utils.py:1096 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}


[INFO|2024-12-04 07:50:25] modeling_utils.py:4800 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.


[INFO|2024-12-04 07:50:25] modeling_utils.py:4808 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.

[INFO|2024-12-04 07:50:25] configuration_utils.py:1049 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/generation_config.json

[INFO|2024-12-04 07:50:25] configuration_utils.py:1096 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}


[INFO|2024-12-04 07:50:25] logging.py:157 >> Gradient checkpointing enabled.

[INFO|2024-12-04 07:50:25] logging.py:157 >> Using torch SDPA for faster training and inference.

[INFO|2024-12-04 07:50:25] logging.py:157 >> Upcasting trainable params to float32.

[INFO|2024-12-04 07:50:25] logging.py:157 >> Fine-tuning method: LoRA

[INFO|2024-12-04 07:50:25] logging.py:157 >> Found linear modules: v_proj,o_proj,down_proj,q_proj,k_proj,gate_proj,up_proj

[INFO|2024-12-04 07:50:26] logging.py:157 >> trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643

[INFO|2024-12-04 07:50:26] trainer.py:2313 >> ***** Running training *****

[INFO|2024-12-04 07:50:26] trainer.py:2314 >>   Num examples = 8,499

[INFO|2024-12-04 07:50:26] trainer.py:2315 >>   Num Epochs = 3

[INFO|2024-12-04 07:50:26] trainer.py:2316 >>   Instantaneous batch size per device = 2

[INFO|2024-12-04 07:50:26] trainer.py:2319 >>   Total train batch size (w. parallel, distributed & accumulation) = 32

[INFO|2024-12-04 07:50:26] trainer.py:2320 >>   Gradient Accumulation steps = 8

[INFO|2024-12-04 07:50:26] trainer.py:2321 >>   Total optimization steps = 795

[INFO|2024-12-04 07:50:26] trainer.py:2322 >>   Number of trainable parameters = 20,185,088

[INFO|2024-12-04 07:50:42] logging.py:157 >> {'loss': 2.6349, 'learning_rate': 4.9995e-05, 'epoch': 0.02}

[INFO|2024-12-04 07:50:55] logging.py:157 >> {'loss': 2.1695, 'learning_rate': 4.9980e-05, 'epoch': 0.04}

[INFO|2024-12-04 07:51:10] logging.py:157 >> {'loss': 2.0415, 'learning_rate': 4.9956e-05, 'epoch': 0.06}

[INFO|2024-12-04 07:51:24] logging.py:157 >> {'loss': 2.0828, 'learning_rate': 4.9922e-05, 'epoch': 0.08}

[INFO|2024-12-04 07:51:38] logging.py:157 >> {'loss': 1.9991, 'learning_rate': 4.9878e-05, 'epoch': 0.09}

[INFO|2024-12-04 07:51:52] logging.py:157 >> {'loss': 1.6721, 'learning_rate': 4.9825e-05, 'epoch': 0.11}

[INFO|2024-12-04 07:52:06] logging.py:157 >> {'loss': 1.9688, 'learning_rate': 4.9761e-05, 'epoch': 0.13}

[INFO|2024-12-04 07:52:19] logging.py:157 >> {'loss': 1.7099, 'learning_rate': 4.9688e-05, 'epoch': 0.15}

[INFO|2024-12-04 07:52:34] logging.py:157 >> {'loss': 2.1824, 'learning_rate': 4.9606e-05, 'epoch': 0.17}

[INFO|2024-12-04 07:52:47] logging.py:157 >> {'loss': 2.1004, 'learning_rate': 4.9514e-05, 'epoch': 0.19}

[INFO|2024-12-04 07:53:01] logging.py:157 >> {'loss': 2.0324, 'learning_rate': 4.9412e-05, 'epoch': 0.21}

[INFO|2024-12-04 07:53:15] logging.py:157 >> {'loss': 1.7394, 'learning_rate': 4.9301e-05, 'epoch': 0.23}

[INFO|2024-12-04 07:53:29] logging.py:157 >> {'loss': 2.0137, 'learning_rate': 4.9180e-05, 'epoch': 0.24}

[INFO|2024-12-04 07:53:44] logging.py:157 >> {'loss': 1.9739, 'learning_rate': 4.9050e-05, 'epoch': 0.26}

[INFO|2024-12-04 07:53:59] logging.py:157 >> {'loss': 1.8814, 'learning_rate': 4.8910e-05, 'epoch': 0.28}

[INFO|2024-12-04 07:54:14] logging.py:157 >> {'loss': 1.7640, 'learning_rate': 4.8761e-05, 'epoch': 0.30}

[INFO|2024-12-04 07:54:29] logging.py:157 >> {'loss': 1.8826, 'learning_rate': 4.8603e-05, 'epoch': 0.32}

[INFO|2024-12-04 07:54:43] logging.py:157 >> {'loss': 2.0176, 'learning_rate': 4.8435e-05, 'epoch': 0.34}

[INFO|2024-12-04 07:54:58] logging.py:157 >> {'loss': 2.0028, 'learning_rate': 4.8259e-05, 'epoch': 0.36}

[INFO|2024-12-04 07:55:13] logging.py:157 >> {'loss': 1.9742, 'learning_rate': 4.8073e-05, 'epoch': 0.38}

[INFO|2024-12-04 07:55:13] trainer.py:3801 >> Saving model checkpoint to saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-100

[INFO|2024-12-04 07:55:13] configuration_utils.py:677 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/config.json

[INFO|2024-12-04 07:55:13] configuration_utils.py:746 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


[INFO|2024-12-04 07:55:14] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-100/tokenizer_config.json

[INFO|2024-12-04 07:55:14] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-100/special_tokens_map.json

[INFO|2024-12-04 07:55:28] logging.py:157 >> {'loss': 1.8273, 'learning_rate': 4.7879e-05, 'epoch': 0.40}

[INFO|2024-12-04 07:55:43] logging.py:157 >> {'loss': 1.9518, 'learning_rate': 4.7675e-05, 'epoch': 0.41}

[INFO|2024-12-04 07:55:57] logging.py:157 >> {'loss': 2.0129, 'learning_rate': 4.7463e-05, 'epoch': 0.43}

[INFO|2024-12-04 07:56:12] logging.py:157 >> {'loss': 1.9800, 'learning_rate': 4.7241e-05, 'epoch': 0.45}

[INFO|2024-12-04 07:56:25] logging.py:157 >> {'loss': 2.0929, 'learning_rate': 4.7012e-05, 'epoch': 0.47}

[INFO|2024-12-04 07:56:40] logging.py:157 >> {'loss': 1.9142, 'learning_rate': 4.6773e-05, 'epoch': 0.49}

[INFO|2024-12-04 07:56:55] logging.py:157 >> {'loss': 1.7380, 'learning_rate': 4.6526e-05, 'epoch': 0.51}

[INFO|2024-12-04 07:57:08] logging.py:157 >> {'loss': 2.1378, 'learning_rate': 4.6271e-05, 'epoch': 0.53}

[INFO|2024-12-04 07:57:23] logging.py:157 >> {'loss': 1.8986, 'learning_rate': 4.6007e-05, 'epoch': 0.55}

[INFO|2024-12-04 07:57:37] logging.py:157 >> {'loss': 1.9440, 'learning_rate': 4.5735e-05, 'epoch': 0.56}

[INFO|2024-12-04 07:57:50] logging.py:157 >> {'loss': 1.8409, 'learning_rate': 4.5455e-05, 'epoch': 0.58}

[INFO|2024-12-04 07:58:04] logging.py:157 >> {'loss': 2.0119, 'learning_rate': 4.5167e-05, 'epoch': 0.60}

[INFO|2024-12-04 07:58:18] logging.py:157 >> {'loss': 2.0673, 'learning_rate': 4.4871e-05, 'epoch': 0.62}

[INFO|2024-12-04 07:58:32] logging.py:157 >> {'loss': 2.1003, 'learning_rate': 4.4568e-05, 'epoch': 0.64}

[INFO|2024-12-04 07:58:46] logging.py:157 >> {'loss': 1.9538, 'learning_rate': 4.4257e-05, 'epoch': 0.66}

[INFO|2024-12-04 07:59:00] logging.py:157 >> {'loss': 2.1268, 'learning_rate': 4.3938e-05, 'epoch': 0.68}

[INFO|2024-12-04 07:59:14] logging.py:157 >> {'loss': 1.9949, 'learning_rate': 4.3612e-05, 'epoch': 0.70}

[INFO|2024-12-04 07:59:28] logging.py:157 >> {'loss': 1.7138, 'learning_rate': 4.3278e-05, 'epoch': 0.72}

[INFO|2024-12-04 07:59:43] logging.py:157 >> {'loss': 2.0058, 'learning_rate': 4.2938e-05, 'epoch': 0.73}

[INFO|2024-12-04 07:59:57] logging.py:157 >> {'loss': 1.9632, 'learning_rate': 4.2590e-05, 'epoch': 0.75}

[INFO|2024-12-04 07:59:57] trainer.py:3801 >> Saving model checkpoint to saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-200

[INFO|2024-12-04 07:59:57] configuration_utils.py:677 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/config.json

[INFO|2024-12-04 07:59:57] configuration_utils.py:746 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


[INFO|2024-12-04 07:59:58] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-200/tokenizer_config.json

[INFO|2024-12-04 07:59:58] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-200/special_tokens_map.json

[INFO|2024-12-04 08:00:12] logging.py:157 >> {'loss': 1.7627, 'learning_rate': 4.2236e-05, 'epoch': 0.77}

[INFO|2024-12-04 08:00:26] logging.py:157 >> {'loss': 1.8894, 'learning_rate': 4.1875e-05, 'epoch': 0.79}

[INFO|2024-12-04 08:00:40] logging.py:157 >> {'loss': 2.0694, 'learning_rate': 4.1507e-05, 'epoch': 0.81}

[INFO|2024-12-04 08:00:57] logging.py:157 >> {'loss': 2.0102, 'learning_rate': 4.1133e-05, 'epoch': 0.83}

[INFO|2024-12-04 08:01:12] logging.py:157 >> {'loss': 1.9291, 'learning_rate': 4.0752e-05, 'epoch': 0.85}

[INFO|2024-12-04 08:01:26] logging.py:157 >> {'loss': 1.9676, 'learning_rate': 4.0366e-05, 'epoch': 0.87}

[INFO|2024-12-04 08:01:40] logging.py:157 >> {'loss': 1.8173, 'learning_rate': 3.9973e-05, 'epoch': 0.88}

[INFO|2024-12-04 08:01:54] logging.py:157 >> {'loss': 2.0390, 'learning_rate': 3.9574e-05, 'epoch': 0.90}

[INFO|2024-12-04 08:02:08] logging.py:157 >> {'loss': 1.8734, 'learning_rate': 3.9170e-05, 'epoch': 0.92}

[INFO|2024-12-04 08:02:23] logging.py:157 >> {'loss': 1.8649, 'learning_rate': 3.8761e-05, 'epoch': 0.94}

[INFO|2024-12-04 08:02:37] logging.py:157 >> {'loss': 2.0412, 'learning_rate': 3.8346e-05, 'epoch': 0.96}

[INFO|2024-12-04 08:02:52] logging.py:157 >> {'loss': 2.0347, 'learning_rate': 3.7925e-05, 'epoch': 0.98}

[INFO|2024-12-04 08:03:06] logging.py:157 >> {'loss': 1.8500, 'learning_rate': 3.7500e-05, 'epoch': 1.00}

[INFO|2024-12-04 08:03:20] logging.py:157 >> {'loss': 1.8402, 'learning_rate': 3.7070e-05, 'epoch': 1.02}

[INFO|2024-12-04 08:03:34] logging.py:157 >> {'loss': 1.9470, 'learning_rate': 3.6635e-05, 'epoch': 1.04}

[INFO|2024-12-04 08:03:48] logging.py:157 >> {'loss': 1.8532, 'learning_rate': 3.6195e-05, 'epoch': 1.06}

[INFO|2024-12-04 08:04:03] logging.py:157 >> {'loss': 1.7019, 'learning_rate': 3.5752e-05, 'epoch': 1.07}

[INFO|2024-12-04 08:04:17] logging.py:157 >> {'loss': 1.8847, 'learning_rate': 3.5304e-05, 'epoch': 1.09}

[INFO|2024-12-04 08:04:31] logging.py:157 >> {'loss': 2.0420, 'learning_rate': 3.4852e-05, 'epoch': 1.11}

[INFO|2024-12-04 08:04:45] logging.py:157 >> {'loss': 1.7960, 'learning_rate': 3.4396e-05, 'epoch': 1.13}

[INFO|2024-12-04 08:04:45] trainer.py:3801 >> Saving model checkpoint to saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-300

[INFO|2024-12-04 08:04:45] configuration_utils.py:677 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/config.json

[INFO|2024-12-04 08:04:45] configuration_utils.py:746 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


[INFO|2024-12-04 08:04:45] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-300/tokenizer_config.json

[INFO|2024-12-04 08:04:45] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-300/special_tokens_map.json

[INFO|2024-12-04 08:05:00] logging.py:157 >> {'loss': 1.9710, 'learning_rate': 3.3936e-05, 'epoch': 1.15}

[INFO|2024-12-04 08:05:14] logging.py:157 >> {'loss': 1.5248, 'learning_rate': 3.3473e-05, 'epoch': 1.17}

[INFO|2024-12-04 08:05:28] logging.py:157 >> {'loss': 2.0860, 'learning_rate': 3.3007e-05, 'epoch': 1.19}

[INFO|2024-12-04 08:05:42] logging.py:157 >> {'loss': 1.8174, 'learning_rate': 3.2537e-05, 'epoch': 1.21}

[INFO|2024-12-04 08:05:56] logging.py:157 >> {'loss': 1.7909, 'learning_rate': 3.2065e-05, 'epoch': 1.22}

[INFO|2024-12-04 08:06:10] logging.py:157 >> {'loss': 1.8262, 'learning_rate': 3.1590e-05, 'epoch': 1.24}

[INFO|2024-12-04 08:06:24] logging.py:157 >> {'loss': 1.8943, 'learning_rate': 3.1112e-05, 'epoch': 1.26}

[INFO|2024-12-04 08:06:39] logging.py:157 >> {'loss': 1.7469, 'learning_rate': 3.0632e-05, 'epoch': 1.28}

[INFO|2024-12-04 08:06:53] logging.py:157 >> {'loss': 1.7342, 'learning_rate': 3.0149e-05, 'epoch': 1.30}

[INFO|2024-12-04 08:07:07] logging.py:157 >> {'loss': 1.8776, 'learning_rate': 2.9665e-05, 'epoch': 1.32}

[INFO|2024-12-04 08:07:21] logging.py:157 >> {'loss': 1.8152, 'learning_rate': 2.9179e-05, 'epoch': 1.34}

[INFO|2024-12-04 08:07:36] logging.py:157 >> {'loss': 1.8152, 'learning_rate': 2.8691e-05, 'epoch': 1.36}

[INFO|2024-12-04 08:07:49] logging.py:157 >> {'loss': 1.8073, 'learning_rate': 2.8202e-05, 'epoch': 1.38}

[INFO|2024-12-04 08:08:02] logging.py:157 >> {'loss': 1.7547, 'learning_rate': 2.7711e-05, 'epoch': 1.39}

[INFO|2024-12-04 08:08:17] logging.py:157 >> {'loss': 1.6981, 'learning_rate': 2.7220e-05, 'epoch': 1.41}

[INFO|2024-12-04 08:08:32] logging.py:157 >> {'loss': 1.9211, 'learning_rate': 2.6727e-05, 'epoch': 1.43}

[INFO|2024-12-04 08:08:48] logging.py:157 >> {'loss': 1.7343, 'learning_rate': 2.6234e-05, 'epoch': 1.45}

[INFO|2024-12-04 08:09:02] logging.py:157 >> {'loss': 1.7537, 'learning_rate': 2.5741e-05, 'epoch': 1.47}

[INFO|2024-12-04 08:09:16] logging.py:157 >> {'loss': 1.7454, 'learning_rate': 2.5247e-05, 'epoch': 1.49}

[INFO|2024-12-04 08:09:30] logging.py:157 >> {'loss': 1.5538, 'learning_rate': 2.4753e-05, 'epoch': 1.51}

[INFO|2024-12-04 08:09:30] trainer.py:3801 >> Saving model checkpoint to saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-400

[INFO|2024-12-04 08:09:30] configuration_utils.py:677 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/config.json

[INFO|2024-12-04 08:09:30] configuration_utils.py:746 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


[INFO|2024-12-04 08:09:31] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-400/tokenizer_config.json

[INFO|2024-12-04 08:09:31] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-400/special_tokens_map.json

[INFO|2024-12-04 08:09:45] logging.py:157 >> {'loss': 1.8667, 'learning_rate': 2.4259e-05, 'epoch': 1.53}

[INFO|2024-12-04 08:09:59] logging.py:157 >> {'loss': 1.8699, 'learning_rate': 2.3766e-05, 'epoch': 1.54}

[INFO|2024-12-04 08:10:15] logging.py:157 >> {'loss': 1.7607, 'learning_rate': 2.3273e-05, 'epoch': 1.56}

[INFO|2024-12-04 08:10:29] logging.py:157 >> {'loss': 1.7724, 'learning_rate': 2.2780e-05, 'epoch': 1.58}

[INFO|2024-12-04 08:10:43] logging.py:157 >> {'loss': 1.6873, 'learning_rate': 2.2289e-05, 'epoch': 1.60}

[INFO|2024-12-04 08:10:57] logging.py:157 >> {'loss': 1.9552, 'learning_rate': 2.1798e-05, 'epoch': 1.62}

[INFO|2024-12-04 08:11:11] logging.py:157 >> {'loss': 1.6112, 'learning_rate': 2.1309e-05, 'epoch': 1.64}

[INFO|2024-12-04 08:11:26] logging.py:157 >> {'loss': 1.6518, 'learning_rate': 2.0821e-05, 'epoch': 1.66}

[INFO|2024-12-04 08:11:42] logging.py:157 >> {'loss': 1.6461, 'learning_rate': 2.0335e-05, 'epoch': 1.68}

[INFO|2024-12-04 08:11:58] logging.py:157 >> {'loss': 1.8623, 'learning_rate': 1.9851e-05, 'epoch': 1.70}

[INFO|2024-12-04 08:12:12] logging.py:157 >> {'loss': 1.8960, 'learning_rate': 1.9368e-05, 'epoch': 1.71}

[INFO|2024-12-04 08:12:26] logging.py:157 >> {'loss': 1.9178, 'learning_rate': 1.8888e-05, 'epoch': 1.73}

[INFO|2024-12-04 08:12:40] logging.py:157 >> {'loss': 1.7133, 'learning_rate': 1.8410e-05, 'epoch': 1.75}

[INFO|2024-12-04 08:12:53] logging.py:157 >> {'loss': 1.6370, 'learning_rate': 1.7935e-05, 'epoch': 1.77}

[INFO|2024-12-04 08:13:08] logging.py:157 >> {'loss': 1.8793, 'learning_rate': 1.7463e-05, 'epoch': 1.79}

[INFO|2024-12-04 08:13:23] logging.py:157 >> {'loss': 1.8576, 'learning_rate': 1.6993e-05, 'epoch': 1.81}

[INFO|2024-12-04 08:13:37] logging.py:157 >> {'loss': 1.7880, 'learning_rate': 1.6527e-05, 'epoch': 1.83}

[INFO|2024-12-04 08:13:51] logging.py:157 >> {'loss': 1.8463, 'learning_rate': 1.6064e-05, 'epoch': 1.85}

[INFO|2024-12-04 08:14:07] logging.py:157 >> {'loss': 1.7377, 'learning_rate': 1.5604e-05, 'epoch': 1.86}

[INFO|2024-12-04 08:14:22] logging.py:157 >> {'loss': 1.8184, 'learning_rate': 1.5148e-05, 'epoch': 1.88}

[INFO|2024-12-04 08:14:22] trainer.py:3801 >> Saving model checkpoint to saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-500

[INFO|2024-12-04 08:14:22] configuration_utils.py:677 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/config.json

[INFO|2024-12-04 08:14:22] configuration_utils.py:746 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


[INFO|2024-12-04 08:14:22] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-500/tokenizer_config.json

[INFO|2024-12-04 08:14:22] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-500/special_tokens_map.json

[INFO|2024-12-04 08:14:37] logging.py:157 >> {'loss': 1.8095, 'learning_rate': 1.4696e-05, 'epoch': 1.90}

[INFO|2024-12-04 08:14:50] logging.py:157 >> {'loss': 1.7896, 'learning_rate': 1.4248e-05, 'epoch': 1.92}

[INFO|2024-12-04 08:15:04] logging.py:157 >> {'loss': 1.7455, 'learning_rate': 1.3805e-05, 'epoch': 1.94}

[INFO|2024-12-04 08:15:18] logging.py:157 >> {'loss': 1.6587, 'learning_rate': 1.3365e-05, 'epoch': 1.96}

[INFO|2024-12-04 08:15:33] logging.py:157 >> {'loss': 1.8107, 'learning_rate': 1.2930e-05, 'epoch': 1.98}

[INFO|2024-12-04 08:15:47] logging.py:157 >> {'loss': 1.7635, 'learning_rate': 1.2500e-05, 'epoch': 2.00}

[INFO|2024-12-04 08:16:00] logging.py:157 >> {'loss': 1.8336, 'learning_rate': 1.2075e-05, 'epoch': 2.02}

[INFO|2024-12-04 08:16:14] logging.py:157 >> {'loss': 1.5858, 'learning_rate': 1.1654e-05, 'epoch': 2.03}

[INFO|2024-12-04 08:16:28] logging.py:157 >> {'loss': 1.7242, 'learning_rate': 1.1239e-05, 'epoch': 2.05}

[INFO|2024-12-04 08:16:43] logging.py:157 >> {'loss': 1.7634, 'learning_rate': 1.0830e-05, 'epoch': 2.07}

[INFO|2024-12-04 08:16:57] logging.py:157 >> {'loss': 1.5763, 'learning_rate': 1.0426e-05, 'epoch': 2.09}

[INFO|2024-12-04 08:17:11] logging.py:157 >> {'loss': 1.4003, 'learning_rate': 1.0027e-05, 'epoch': 2.11}

[INFO|2024-12-04 08:17:26] logging.py:157 >> {'loss': 1.7338, 'learning_rate': 9.6344e-06, 'epoch': 2.13}

[INFO|2024-12-04 08:17:40] logging.py:157 >> {'loss': 1.3759, 'learning_rate': 9.2478e-06, 'epoch': 2.15}

[INFO|2024-12-04 08:17:54] logging.py:157 >> {'loss': 1.7561, 'learning_rate': 8.8673e-06, 'epoch': 2.17}

[INFO|2024-12-04 08:18:07] logging.py:157 >> {'loss': 1.6728, 'learning_rate': 8.4932e-06, 'epoch': 2.19}

[INFO|2024-12-04 08:18:22] logging.py:157 >> {'loss': 1.5750, 'learning_rate': 8.1254e-06, 'epoch': 2.20}

[INFO|2024-12-04 08:18:37] logging.py:157 >> {'loss': 1.7916, 'learning_rate': 7.7643e-06, 'epoch': 2.22}

[INFO|2024-12-04 08:18:53] logging.py:157 >> {'loss': 1.7747, 'learning_rate': 7.4099e-06, 'epoch': 2.24}

[INFO|2024-12-04 08:19:06] logging.py:157 >> {'loss': 1.7301, 'learning_rate': 7.0623e-06, 'epoch': 2.26}

[INFO|2024-12-04 08:19:06] trainer.py:3801 >> Saving model checkpoint to saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-600

[INFO|2024-12-04 08:19:06] configuration_utils.py:677 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/config.json

[INFO|2024-12-04 08:19:06] configuration_utils.py:746 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


[INFO|2024-12-04 08:19:06] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-600/tokenizer_config.json

[INFO|2024-12-04 08:19:06] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-600/special_tokens_map.json

[INFO|2024-12-04 08:19:20] logging.py:157 >> {'loss': 1.6077, 'learning_rate': 6.7218e-06, 'epoch': 2.28}

[INFO|2024-12-04 08:19:35] logging.py:157 >> {'loss': 1.6401, 'learning_rate': 6.3884e-06, 'epoch': 2.30}

[INFO|2024-12-04 08:19:49] logging.py:157 >> {'loss': 1.5965, 'learning_rate': 6.0622e-06, 'epoch': 2.32}

[INFO|2024-12-04 08:20:02] logging.py:157 >> {'loss': 1.7680, 'learning_rate': 5.7435e-06, 'epoch': 2.34}

[INFO|2024-12-04 08:20:16] logging.py:157 >> {'loss': 1.4631, 'learning_rate': 5.4322e-06, 'epoch': 2.35}

[INFO|2024-12-04 08:20:31] logging.py:157 >> {'loss': 1.7724, 'learning_rate': 5.1286e-06, 'epoch': 2.37}

[INFO|2024-12-04 08:20:44] logging.py:157 >> {'loss': 1.6654, 'learning_rate': 4.8328e-06, 'epoch': 2.39}

[INFO|2024-12-04 08:20:58] logging.py:157 >> {'loss': 1.8716, 'learning_rate': 4.5448e-06, 'epoch': 2.41}

[INFO|2024-12-04 08:21:13] logging.py:157 >> {'loss': 1.7749, 'learning_rate': 4.2649e-06, 'epoch': 2.43}

[INFO|2024-12-04 08:21:28] logging.py:157 >> {'loss': 1.5521, 'learning_rate': 3.9930e-06, 'epoch': 2.45}

[INFO|2024-12-04 08:21:42] logging.py:157 >> {'loss': 1.7111, 'learning_rate': 3.7293e-06, 'epoch': 2.47}

[INFO|2024-12-04 08:21:58] logging.py:157 >> {'loss': 1.5971, 'learning_rate': 3.4739e-06, 'epoch': 2.49}

[INFO|2024-12-04 08:22:12] logging.py:157 >> {'loss': 1.6342, 'learning_rate': 3.2269e-06, 'epoch': 2.51}

[INFO|2024-12-04 08:22:26] logging.py:157 >> {'loss': 1.8203, 'learning_rate': 2.9885e-06, 'epoch': 2.52}

[INFO|2024-12-04 08:22:41] logging.py:157 >> {'loss': 1.5745, 'learning_rate': 2.7586e-06, 'epoch': 2.54}

[INFO|2024-12-04 08:22:54] logging.py:157 >> {'loss': 1.5390, 'learning_rate': 2.5374e-06, 'epoch': 2.56}

[INFO|2024-12-04 08:23:08] logging.py:157 >> {'loss': 1.5527, 'learning_rate': 2.3249e-06, 'epoch': 2.58}

[INFO|2024-12-04 08:23:24] logging.py:157 >> {'loss': 1.6538, 'learning_rate': 2.1214e-06, 'epoch': 2.60}

[INFO|2024-12-04 08:23:38] logging.py:157 >> {'loss': 1.6903, 'learning_rate': 1.9267e-06, 'epoch': 2.62}

[INFO|2024-12-04 08:23:54] logging.py:157 >> {'loss': 1.7311, 'learning_rate': 1.7411e-06, 'epoch': 2.64}

[INFO|2024-12-04 08:23:54] trainer.py:3801 >> Saving model checkpoint to saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-700

[INFO|2024-12-04 08:23:54] configuration_utils.py:677 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/config.json

[INFO|2024-12-04 08:23:54] configuration_utils.py:746 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


[INFO|2024-12-04 08:23:54] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-700/tokenizer_config.json

[INFO|2024-12-04 08:23:54] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-700/special_tokens_map.json

[INFO|2024-12-04 08:24:10] logging.py:157 >> {'loss': 1.7871, 'learning_rate': 1.5645e-06, 'epoch': 2.66}

[INFO|2024-12-04 08:24:24] logging.py:157 >> {'loss': 1.6927, 'learning_rate': 1.3971e-06, 'epoch': 2.67}

[INFO|2024-12-04 08:24:39] logging.py:157 >> {'loss': 1.6636, 'learning_rate': 1.2389e-06, 'epoch': 2.69}

[INFO|2024-12-04 08:24:53] logging.py:157 >> {'loss': 1.6511, 'learning_rate': 1.0900e-06, 'epoch': 2.71}

[INFO|2024-12-04 08:25:07] logging.py:157 >> {'loss': 1.5396, 'learning_rate': 9.5039e-07, 'epoch': 2.73}

[INFO|2024-12-04 08:25:21] logging.py:157 >> {'loss': 1.5506, 'learning_rate': 8.2019e-07, 'epoch': 2.75}

[INFO|2024-12-04 08:25:37] logging.py:157 >> {'loss': 1.6512, 'learning_rate': 6.9943e-07, 'epoch': 2.77}

[INFO|2024-12-04 08:25:52] logging.py:157 >> {'loss': 1.5012, 'learning_rate': 5.8815e-07, 'epoch': 2.79}

[INFO|2024-12-04 08:26:06] logging.py:157 >> {'loss': 1.6716, 'learning_rate': 4.8641e-07, 'epoch': 2.81}

[INFO|2024-12-04 08:26:20] logging.py:157 >> {'loss': 1.5450, 'learning_rate': 3.9424e-07, 'epoch': 2.83}

[INFO|2024-12-04 08:26:34] logging.py:157 >> {'loss': 1.7720, 'learning_rate': 3.1167e-07, 'epoch': 2.84}

[INFO|2024-12-04 08:26:48] logging.py:157 >> {'loss': 1.7754, 'learning_rate': 2.3874e-07, 'epoch': 2.86}

[INFO|2024-12-04 08:27:01] logging.py:157 >> {'loss': 1.7166, 'learning_rate': 1.7547e-07, 'epoch': 2.88}

[INFO|2024-12-04 08:27:17] logging.py:157 >> {'loss': 1.9639, 'learning_rate': 1.2190e-07, 'epoch': 2.90}

[INFO|2024-12-04 08:27:31] logging.py:157 >> {'loss': 1.6908, 'learning_rate': 7.8039e-08, 'epoch': 2.92}

[INFO|2024-12-04 08:27:45] logging.py:157 >> {'loss': 1.6639, 'learning_rate': 4.3907e-08, 'epoch': 2.94}

[INFO|2024-12-04 08:28:01] logging.py:157 >> {'loss': 1.6658, 'learning_rate': 1.9517e-08, 'epoch': 2.96}

[INFO|2024-12-04 08:28:15] logging.py:157 >> {'loss': 1.7916, 'learning_rate': 4.8798e-09, 'epoch': 2.98}

[INFO|2024-12-04 08:28:30] logging.py:157 >> {'loss': 1.6652, 'learning_rate': 0.0000e+00, 'epoch': 2.99}

[INFO|2024-12-04 08:28:30] trainer.py:3801 >> Saving model checkpoint to saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-795

[INFO|2024-12-04 08:28:30] configuration_utils.py:677 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/config.json

[INFO|2024-12-04 08:28:30] configuration_utils.py:746 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


[INFO|2024-12-04 08:28:31] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-795/tokenizer_config.json

[INFO|2024-12-04 08:28:31] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/checkpoint-795/special_tokens_map.json

[INFO|2024-12-04 08:28:31] trainer.py:2584 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)



[INFO|2024-12-04 08:28:31] trainer.py:3801 >> Saving model checkpoint to saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25

[INFO|2024-12-04 08:28:31] configuration_utils.py:677 >> loading configuration file /root/zjj/xjd/huawei-ict-2024/ChatStyle/.mindnlp/model/Qwen/Qwen2-7B-Instruct/config.json

[INFO|2024-12-04 08:28:31] configuration_utils.py:746 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


[INFO|2024-12-04 08:28:31] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/tokenizer_config.json

[INFO|2024-12-04 08:28:31] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/Qwen2-7B-Instruct/lora/train_2024-12-04-07-41-25/special_tokens_map.json

[WARNING|2024-12-04 08:28:32] logging.py:162 >> No metric eval_loss to plot.

[WARNING|2024-12-04 08:28:32] logging.py:162 >> No metric eval_accuracy to plot.

[INFO|2024-12-04 08:28:32] modelcard.py:449 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

