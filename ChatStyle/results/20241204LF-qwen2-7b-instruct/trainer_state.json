{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9948235294117644,
  "eval_steps": 500,
  "global_step": 795,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018823529411764704,
      "grad_norm": 1.8814473152160645,
      "learning_rate": 4.999512020721228e-05,
      "loss": 2.6349,
      "step": 5
    },
    {
      "epoch": 0.03764705882352941,
      "grad_norm": 1.1726794242858887,
      "learning_rate": 4.99804827338393e-05,
      "loss": 2.1695,
      "step": 10
    },
    {
      "epoch": 0.05647058823529412,
      "grad_norm": 1.3939763307571411,
      "learning_rate": 4.995609329410804e-05,
      "loss": 2.0415,
      "step": 15
    },
    {
      "epoch": 0.07529411764705882,
      "grad_norm": 1.3511627912521362,
      "learning_rate": 4.9921961409251464e-05,
      "loss": 2.0828,
      "step": 20
    },
    {
      "epoch": 0.09411764705882353,
      "grad_norm": 0.9660437703132629,
      "learning_rate": 4.98781004037916e-05,
      "loss": 1.9991,
      "step": 25
    },
    {
      "epoch": 0.11294117647058824,
      "grad_norm": 0.9256454110145569,
      "learning_rate": 4.982452740033793e-05,
      "loss": 1.6721,
      "step": 30
    },
    {
      "epoch": 0.13176470588235295,
      "grad_norm": 1.3718727827072144,
      "learning_rate": 4.9761263312902895e-05,
      "loss": 1.9688,
      "step": 35
    },
    {
      "epoch": 0.15058823529411763,
      "grad_norm": 1.426705002784729,
      "learning_rate": 4.9688332838737504e-05,
      "loss": 1.7099,
      "step": 40
    },
    {
      "epoch": 0.16941176470588235,
      "grad_norm": 1.221659779548645,
      "learning_rate": 4.960576444868992e-05,
      "loss": 2.1824,
      "step": 45
    },
    {
      "epoch": 0.18823529411764706,
      "grad_norm": 1.356835961341858,
      "learning_rate": 4.951359037609088e-05,
      "loss": 2.1004,
      "step": 50
    },
    {
      "epoch": 0.20705882352941177,
      "grad_norm": 1.0937025547027588,
      "learning_rate": 4.9411846604170345e-05,
      "loss": 2.0324,
      "step": 55
    },
    {
      "epoch": 0.22588235294117648,
      "grad_norm": 1.1398603916168213,
      "learning_rate": 4.930057285201027e-05,
      "loss": 1.7394,
      "step": 60
    },
    {
      "epoch": 0.2447058823529412,
      "grad_norm": 1.2447240352630615,
      "learning_rate": 4.917981255903893e-05,
      "loss": 2.0137,
      "step": 65
    },
    {
      "epoch": 0.2635294117647059,
      "grad_norm": 1.272847056388855,
      "learning_rate": 4.9049612868072844e-05,
      "loss": 1.9739,
      "step": 70
    },
    {
      "epoch": 0.2823529411764706,
      "grad_norm": 1.2050793170928955,
      "learning_rate": 4.891002460691306e-05,
      "loss": 1.8814,
      "step": 75
    },
    {
      "epoch": 0.30117647058823527,
      "grad_norm": 1.2590155601501465,
      "learning_rate": 4.876110226850278e-05,
      "loss": 1.764,
      "step": 80
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.164302110671997,
      "learning_rate": 4.8602903989654224e-05,
      "loss": 1.8826,
      "step": 85
    },
    {
      "epoch": 0.3388235294117647,
      "grad_norm": 1.1629287004470825,
      "learning_rate": 4.8435491528353026e-05,
      "loss": 2.0176,
      "step": 90
    },
    {
      "epoch": 0.35764705882352943,
      "grad_norm": 0.9843223094940186,
      "learning_rate": 4.8258930239648865e-05,
      "loss": 2.0028,
      "step": 95
    },
    {
      "epoch": 0.3764705882352941,
      "grad_norm": 1.252121090888977,
      "learning_rate": 4.807328905014201e-05,
      "loss": 1.9742,
      "step": 100
    },
    {
      "epoch": 0.3952941176470588,
      "grad_norm": 1.4342831373214722,
      "learning_rate": 4.787864043107546e-05,
      "loss": 1.8273,
      "step": 105
    },
    {
      "epoch": 0.41411764705882353,
      "grad_norm": 1.2232533693313599,
      "learning_rate": 4.767506037004344e-05,
      "loss": 1.9518,
      "step": 110
    },
    {
      "epoch": 0.4329411764705882,
      "grad_norm": 1.140058994293213,
      "learning_rate": 4.7462628341326995e-05,
      "loss": 2.0129,
      "step": 115
    },
    {
      "epoch": 0.45176470588235296,
      "grad_norm": 1.4283523559570312,
      "learning_rate": 4.724142727486869e-05,
      "loss": 1.98,
      "step": 120
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 1.292124629020691,
      "learning_rate": 4.7011543523897996e-05,
      "loss": 2.0929,
      "step": 125
    },
    {
      "epoch": 0.4894117647058824,
      "grad_norm": 1.6443870067596436,
      "learning_rate": 4.677306683122054e-05,
      "loss": 1.9142,
      "step": 130
    },
    {
      "epoch": 0.508235294117647,
      "grad_norm": 1.4119305610656738,
      "learning_rate": 4.652609029418389e-05,
      "loss": 1.738,
      "step": 135
    },
    {
      "epoch": 0.5270588235294118,
      "grad_norm": 1.404895305633545,
      "learning_rate": 4.6270710328334004e-05,
      "loss": 2.1378,
      "step": 140
    },
    {
      "epoch": 0.5458823529411765,
      "grad_norm": 1.3148672580718994,
      "learning_rate": 4.6007026629776104e-05,
      "loss": 1.8986,
      "step": 145
    },
    {
      "epoch": 0.5647058823529412,
      "grad_norm": 1.2483198642730713,
      "learning_rate": 4.573514213625505e-05,
      "loss": 1.944,
      "step": 150
    },
    {
      "epoch": 0.5835294117647059,
      "grad_norm": 1.3273078203201294,
      "learning_rate": 4.545516298697006e-05,
      "loss": 1.8409,
      "step": 155
    },
    {
      "epoch": 0.6023529411764705,
      "grad_norm": 1.4462627172470093,
      "learning_rate": 4.5167198481139825e-05,
      "loss": 2.0119,
      "step": 160
    },
    {
      "epoch": 0.6211764705882353,
      "grad_norm": 1.0897130966186523,
      "learning_rate": 4.4871361035333836e-05,
      "loss": 2.0673,
      "step": 165
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.3354123830795288,
      "learning_rate": 4.456776613958683e-05,
      "loss": 2.1003,
      "step": 170
    },
    {
      "epoch": 0.6588235294117647,
      "grad_norm": 1.1016685962677002,
      "learning_rate": 4.425653231231344e-05,
      "loss": 1.9538,
      "step": 175
    },
    {
      "epoch": 0.6776470588235294,
      "grad_norm": 1.2240774631500244,
      "learning_rate": 4.3937781054040505e-05,
      "loss": 2.1268,
      "step": 180
    },
    {
      "epoch": 0.6964705882352941,
      "grad_norm": 1.360371470451355,
      "learning_rate": 4.361163679997532e-05,
      "loss": 1.9949,
      "step": 185
    },
    {
      "epoch": 0.7152941176470589,
      "grad_norm": 1.3495583534240723,
      "learning_rate": 4.327822687142819e-05,
      "loss": 1.7138,
      "step": 190
    },
    {
      "epoch": 0.7341176470588235,
      "grad_norm": 1.0619421005249023,
      "learning_rate": 4.293768142610828e-05,
      "loss": 2.0058,
      "step": 195
    },
    {
      "epoch": 0.7529411764705882,
      "grad_norm": 1.4458342790603638,
      "learning_rate": 4.259013340731224e-05,
      "loss": 1.9632,
      "step": 200
    },
    {
      "epoch": 0.7717647058823529,
      "grad_norm": 1.6168420314788818,
      "learning_rate": 4.22357184920253e-05,
      "loss": 1.7627,
      "step": 205
    },
    {
      "epoch": 0.7905882352941176,
      "grad_norm": 1.1043998003005981,
      "learning_rate": 4.187457503795527e-05,
      "loss": 1.8894,
      "step": 210
    },
    {
      "epoch": 0.8094117647058824,
      "grad_norm": 1.1894928216934204,
      "learning_rate": 4.150684402951994e-05,
      "loss": 2.0694,
      "step": 215
    },
    {
      "epoch": 0.8282352941176471,
      "grad_norm": 1.1758555173873901,
      "learning_rate": 4.1132669022809136e-05,
      "loss": 2.0102,
      "step": 220
    },
    {
      "epoch": 0.8470588235294118,
      "grad_norm": 1.113900899887085,
      "learning_rate": 4.075219608954278e-05,
      "loss": 1.9291,
      "step": 225
    },
    {
      "epoch": 0.8658823529411764,
      "grad_norm": 1.2071565389633179,
      "learning_rate": 4.036557376004694e-05,
      "loss": 1.9676,
      "step": 230
    },
    {
      "epoch": 0.8847058823529412,
      "grad_norm": 1.3387326002120972,
      "learning_rate": 3.9972952965270006e-05,
      "loss": 1.8173,
      "step": 235
    },
    {
      "epoch": 0.9035294117647059,
      "grad_norm": 1.368204116821289,
      "learning_rate": 3.95744869778618e-05,
      "loss": 2.039,
      "step": 240
    },
    {
      "epoch": 0.9223529411764706,
      "grad_norm": 1.2942441701889038,
      "learning_rate": 3.917033135233845e-05,
      "loss": 1.8734,
      "step": 245
    },
    {
      "epoch": 0.9411764705882353,
      "grad_norm": 1.1002624034881592,
      "learning_rate": 3.876064386435646e-05,
      "loss": 1.8649,
      "step": 250
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.2137409448623657,
      "learning_rate": 3.8345584449119776e-05,
      "loss": 2.0412,
      "step": 255
    },
    {
      "epoch": 0.9788235294117648,
      "grad_norm": 1.1044129133224487,
      "learning_rate": 3.7925315138943655e-05,
      "loss": 2.0347,
      "step": 260
    },
    {
      "epoch": 0.9976470588235294,
      "grad_norm": 1.4145333766937256,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 1.85,
      "step": 265
    },
    {
      "epoch": 1.0174117647058825,
      "grad_norm": 1.1870800256729126,
      "learning_rate": 3.706980506826863e-05,
      "loss": 1.8402,
      "step": 270
    },
    {
      "epoch": 1.0362352941176471,
      "grad_norm": 1.1153630018234253,
      "learning_rate": 3.663489828471953e-05,
      "loss": 1.947,
      "step": 275
    },
    {
      "epoch": 1.0550588235294118,
      "grad_norm": 1.4044957160949707,
      "learning_rate": 3.619544942975158e-05,
      "loss": 1.8532,
      "step": 280
    },
    {
      "epoch": 1.0738823529411765,
      "grad_norm": 1.508392095565796,
      "learning_rate": 3.575163005691302e-05,
      "loss": 1.7019,
      "step": 285
    },
    {
      "epoch": 1.0927058823529412,
      "grad_norm": 1.6188621520996094,
      "learning_rate": 3.530361342592981e-05,
      "loss": 1.8847,
      "step": 290
    },
    {
      "epoch": 1.1115294117647059,
      "grad_norm": 1.6559005975723267,
      "learning_rate": 3.485157443506792e-05,
      "loss": 2.042,
      "step": 295
    },
    {
      "epoch": 1.1303529411764706,
      "grad_norm": 1.588422179222107,
      "learning_rate": 3.4395689552855955e-05,
      "loss": 1.796,
      "step": 300
    },
    {
      "epoch": 1.1491764705882352,
      "grad_norm": 1.7291529178619385,
      "learning_rate": 3.393613674919473e-05,
      "loss": 1.971,
      "step": 305
    },
    {
      "epoch": 1.168,
      "grad_norm": 1.7769404649734497,
      "learning_rate": 3.3473095425880796e-05,
      "loss": 1.5248,
      "step": 310
    },
    {
      "epoch": 1.1868235294117646,
      "grad_norm": 1.5498448610305786,
      "learning_rate": 3.300674634657094e-05,
      "loss": 2.086,
      "step": 315
    },
    {
      "epoch": 1.2056470588235295,
      "grad_norm": 2.2219021320343018,
      "learning_rate": 3.2537271566215076e-05,
      "loss": 1.8174,
      "step": 320
    },
    {
      "epoch": 1.2244705882352942,
      "grad_norm": 2.165092945098877,
      "learning_rate": 3.206485435998498e-05,
      "loss": 1.7909,
      "step": 325
    },
    {
      "epoch": 1.2432941176470589,
      "grad_norm": 1.5129060745239258,
      "learning_rate": 3.158967915172669e-05,
      "loss": 1.8262,
      "step": 330
    },
    {
      "epoch": 1.2621176470588236,
      "grad_norm": 1.6172641515731812,
      "learning_rate": 3.111193144196457e-05,
      "loss": 1.8943,
      "step": 335
    },
    {
      "epoch": 1.2809411764705882,
      "grad_norm": 2.2715344429016113,
      "learning_rate": 3.063179773548487e-05,
      "loss": 1.7469,
      "step": 340
    },
    {
      "epoch": 1.299764705882353,
      "grad_norm": 1.876242756843567,
      "learning_rate": 3.014946546852746e-05,
      "loss": 1.7342,
      "step": 345
    },
    {
      "epoch": 1.3185882352941176,
      "grad_norm": 2.106494188308716,
      "learning_rate": 2.9665122935613727e-05,
      "loss": 1.8776,
      "step": 350
    },
    {
      "epoch": 1.3374117647058823,
      "grad_norm": 1.9568883180618286,
      "learning_rate": 2.917895921603958e-05,
      "loss": 1.8152,
      "step": 355
    },
    {
      "epoch": 1.356235294117647,
      "grad_norm": 2.0248866081237793,
      "learning_rate": 2.8691164100062034e-05,
      "loss": 1.8152,
      "step": 360
    },
    {
      "epoch": 1.3750588235294119,
      "grad_norm": 1.9715206623077393,
      "learning_rate": 2.820192801480817e-05,
      "loss": 1.8073,
      "step": 365
    },
    {
      "epoch": 1.3938823529411764,
      "grad_norm": 2.382704973220825,
      "learning_rate": 2.7711441949935642e-05,
      "loss": 1.7547,
      "step": 370
    },
    {
      "epoch": 1.4127058823529413,
      "grad_norm": 1.8685150146484375,
      "learning_rate": 2.7219897383073373e-05,
      "loss": 1.6981,
      "step": 375
    },
    {
      "epoch": 1.431529411764706,
      "grad_norm": 2.5956764221191406,
      "learning_rate": 2.672748620507195e-05,
      "loss": 1.9211,
      "step": 380
    },
    {
      "epoch": 1.4503529411764706,
      "grad_norm": 1.6988651752471924,
      "learning_rate": 2.623440064509258e-05,
      "loss": 1.7343,
      "step": 385
    },
    {
      "epoch": 1.4691764705882353,
      "grad_norm": 2.68023419380188,
      "learning_rate": 2.5740833195563996e-05,
      "loss": 1.7537,
      "step": 390
    },
    {
      "epoch": 1.488,
      "grad_norm": 2.3903841972351074,
      "learning_rate": 2.5246976537036644e-05,
      "loss": 1.7454,
      "step": 395
    },
    {
      "epoch": 1.5068235294117647,
      "grad_norm": 1.69377601146698,
      "learning_rate": 2.475302346296336e-05,
      "loss": 1.5538,
      "step": 400
    },
    {
      "epoch": 1.5256470588235294,
      "grad_norm": 2.5813934803009033,
      "learning_rate": 2.4259166804436006e-05,
      "loss": 1.8667,
      "step": 405
    },
    {
      "epoch": 1.5444705882352943,
      "grad_norm": 2.5006306171417236,
      "learning_rate": 2.3765599354907427e-05,
      "loss": 1.8699,
      "step": 410
    },
    {
      "epoch": 1.5632941176470587,
      "grad_norm": 2.495293378829956,
      "learning_rate": 2.3272513794928054e-05,
      "loss": 1.7607,
      "step": 415
    },
    {
      "epoch": 1.5821176470588236,
      "grad_norm": 2.2066829204559326,
      "learning_rate": 2.2780102616926633e-05,
      "loss": 1.7724,
      "step": 420
    },
    {
      "epoch": 1.600941176470588,
      "grad_norm": 2.493035316467285,
      "learning_rate": 2.2288558050064367e-05,
      "loss": 1.6873,
      "step": 425
    },
    {
      "epoch": 1.619764705882353,
      "grad_norm": 2.823951482772827,
      "learning_rate": 2.1798071985191832e-05,
      "loss": 1.9552,
      "step": 430
    },
    {
      "epoch": 1.6385882352941177,
      "grad_norm": 2.344181776046753,
      "learning_rate": 2.1308835899937972e-05,
      "loss": 1.6112,
      "step": 435
    },
    {
      "epoch": 1.6574117647058824,
      "grad_norm": 2.3523311614990234,
      "learning_rate": 2.0821040783960423e-05,
      "loss": 1.6518,
      "step": 440
    },
    {
      "epoch": 1.676235294117647,
      "grad_norm": 2.3660762310028076,
      "learning_rate": 2.0334877064386276e-05,
      "loss": 1.6461,
      "step": 445
    },
    {
      "epoch": 1.6950588235294117,
      "grad_norm": 2.137377977371216,
      "learning_rate": 1.9850534531472546e-05,
      "loss": 1.8623,
      "step": 450
    },
    {
      "epoch": 1.7138823529411766,
      "grad_norm": 2.083076238632202,
      "learning_rate": 1.936820226451513e-05,
      "loss": 1.896,
      "step": 455
    },
    {
      "epoch": 1.732705882352941,
      "grad_norm": 2.46024751663208,
      "learning_rate": 1.8888068558035435e-05,
      "loss": 1.9178,
      "step": 460
    },
    {
      "epoch": 1.751529411764706,
      "grad_norm": 2.6472997665405273,
      "learning_rate": 1.8410320848273315e-05,
      "loss": 1.7133,
      "step": 465
    },
    {
      "epoch": 1.7703529411764705,
      "grad_norm": 2.7714169025421143,
      "learning_rate": 1.793514564001503e-05,
      "loss": 1.637,
      "step": 470
    },
    {
      "epoch": 1.7891764705882354,
      "grad_norm": 2.644860029220581,
      "learning_rate": 1.746272843378493e-05,
      "loss": 1.8793,
      "step": 475
    },
    {
      "epoch": 1.808,
      "grad_norm": 2.768531322479248,
      "learning_rate": 1.6993253653429063e-05,
      "loss": 1.8576,
      "step": 480
    },
    {
      "epoch": 1.8268235294117647,
      "grad_norm": 3.0561275482177734,
      "learning_rate": 1.6526904574119213e-05,
      "loss": 1.788,
      "step": 485
    },
    {
      "epoch": 1.8456470588235294,
      "grad_norm": 2.571908473968506,
      "learning_rate": 1.606386325080528e-05,
      "loss": 1.8463,
      "step": 490
    },
    {
      "epoch": 1.864470588235294,
      "grad_norm": 2.309345006942749,
      "learning_rate": 1.560431044714405e-05,
      "loss": 1.7377,
      "step": 495
    },
    {
      "epoch": 1.8832941176470588,
      "grad_norm": 2.7421422004699707,
      "learning_rate": 1.5148425564932084e-05,
      "loss": 1.8184,
      "step": 500
    },
    {
      "epoch": 1.9021176470588235,
      "grad_norm": 2.5012407302856445,
      "learning_rate": 1.4696386574070204e-05,
      "loss": 1.8095,
      "step": 505
    },
    {
      "epoch": 1.9209411764705884,
      "grad_norm": 2.644399881362915,
      "learning_rate": 1.4248369943086998e-05,
      "loss": 1.7896,
      "step": 510
    },
    {
      "epoch": 1.9397647058823528,
      "grad_norm": 2.3981950283050537,
      "learning_rate": 1.3804550570248431e-05,
      "loss": 1.7455,
      "step": 515
    },
    {
      "epoch": 1.9585882352941177,
      "grad_norm": 2.5358054637908936,
      "learning_rate": 1.3365101715280473e-05,
      "loss": 1.6587,
      "step": 520
    },
    {
      "epoch": 1.9774117647058822,
      "grad_norm": 2.5036299228668213,
      "learning_rate": 1.2930194931731382e-05,
      "loss": 1.8107,
      "step": 525
    },
    {
      "epoch": 1.996235294117647,
      "grad_norm": 2.9644925594329834,
      "learning_rate": 1.2500000000000006e-05,
      "loss": 1.7635,
      "step": 530
    },
    {
      "epoch": 2.016,
      "grad_norm": 2.657907009124756,
      "learning_rate": 1.207468486105636e-05,
      "loss": 1.8336,
      "step": 535
    },
    {
      "epoch": 2.034823529411765,
      "grad_norm": 2.5352020263671875,
      "learning_rate": 1.1654415550880243e-05,
      "loss": 1.5858,
      "step": 540
    },
    {
      "epoch": 2.0536470588235294,
      "grad_norm": 2.6786208152770996,
      "learning_rate": 1.1239356135643545e-05,
      "loss": 1.7242,
      "step": 545
    },
    {
      "epoch": 2.0724705882352943,
      "grad_norm": 2.6024200916290283,
      "learning_rate": 1.0829668647661559e-05,
      "loss": 1.7634,
      "step": 550
    },
    {
      "epoch": 2.0912941176470587,
      "grad_norm": 2.8086071014404297,
      "learning_rate": 1.0425513022138203e-05,
      "loss": 1.5763,
      "step": 555
    },
    {
      "epoch": 2.1101176470588237,
      "grad_norm": 2.947167158126831,
      "learning_rate": 1.002704703473e-05,
      "loss": 1.4003,
      "step": 560
    },
    {
      "epoch": 2.128941176470588,
      "grad_norm": 2.4877841472625732,
      "learning_rate": 9.634426239953073e-06,
      "loss": 1.7338,
      "step": 565
    },
    {
      "epoch": 2.147764705882353,
      "grad_norm": 2.539809465408325,
      "learning_rate": 9.247803910457226e-06,
      "loss": 1.3759,
      "step": 570
    },
    {
      "epoch": 2.1665882352941175,
      "grad_norm": 2.3853352069854736,
      "learning_rate": 8.867330977190877e-06,
      "loss": 1.7561,
      "step": 575
    },
    {
      "epoch": 2.1854117647058824,
      "grad_norm": 2.4197850227355957,
      "learning_rate": 8.493155970480073e-06,
      "loss": 1.6728,
      "step": 580
    },
    {
      "epoch": 2.2042352941176473,
      "grad_norm": 3.042753219604492,
      "learning_rate": 8.125424962044742e-06,
      "loss": 1.575,
      "step": 585
    },
    {
      "epoch": 2.2230588235294118,
      "grad_norm": 2.5949206352233887,
      "learning_rate": 7.76428150797471e-06,
      "loss": 1.7916,
      "step": 590
    },
    {
      "epoch": 2.2418823529411767,
      "grad_norm": 2.6232593059539795,
      "learning_rate": 7.409866592687767e-06,
      "loss": 1.7747,
      "step": 595
    },
    {
      "epoch": 2.260705882352941,
      "grad_norm": 2.5679280757904053,
      "learning_rate": 7.062318573891716e-06,
      "loss": 1.7301,
      "step": 600
    },
    {
      "epoch": 2.279529411764706,
      "grad_norm": 2.932805061340332,
      "learning_rate": 6.721773128571812e-06,
      "loss": 1.6077,
      "step": 605
    },
    {
      "epoch": 2.2983529411764705,
      "grad_norm": 2.529676914215088,
      "learning_rate": 6.38836320002468e-06,
      "loss": 1.6401,
      "step": 610
    },
    {
      "epoch": 2.3171764705882354,
      "grad_norm": 2.7949066162109375,
      "learning_rate": 6.062218945959497e-06,
      "loss": 1.5965,
      "step": 615
    },
    {
      "epoch": 2.336,
      "grad_norm": 2.395698308944702,
      "learning_rate": 5.743467687686563e-06,
      "loss": 1.768,
      "step": 620
    },
    {
      "epoch": 2.3548235294117648,
      "grad_norm": 2.4961130619049072,
      "learning_rate": 5.4322338604131715e-06,
      "loss": 1.4631,
      "step": 625
    },
    {
      "epoch": 2.373647058823529,
      "grad_norm": 2.91247296333313,
      "learning_rate": 5.128638964666166e-06,
      "loss": 1.7724,
      "step": 630
    },
    {
      "epoch": 2.392470588235294,
      "grad_norm": 3.0823357105255127,
      "learning_rate": 4.832801518860175e-06,
      "loss": 1.6654,
      "step": 635
    },
    {
      "epoch": 2.411294117647059,
      "grad_norm": 3.2065117359161377,
      "learning_rate": 4.54483701302994e-06,
      "loss": 1.8716,
      "step": 640
    },
    {
      "epoch": 2.4301176470588235,
      "grad_norm": 3.1355161666870117,
      "learning_rate": 4.264857863744956e-06,
      "loss": 1.7749,
      "step": 645
    },
    {
      "epoch": 2.4489411764705884,
      "grad_norm": 3.3205015659332275,
      "learning_rate": 3.992973370223896e-06,
      "loss": 1.5521,
      "step": 650
    },
    {
      "epoch": 2.467764705882353,
      "grad_norm": 3.013352155685425,
      "learning_rate": 3.7292896716659974e-06,
      "loss": 1.7111,
      "step": 655
    },
    {
      "epoch": 2.4865882352941178,
      "grad_norm": 3.2729694843292236,
      "learning_rate": 3.4739097058161114e-06,
      "loss": 1.5971,
      "step": 660
    },
    {
      "epoch": 2.5054117647058822,
      "grad_norm": 3.425785779953003,
      "learning_rate": 3.22693316877947e-06,
      "loss": 1.6342,
      "step": 665
    },
    {
      "epoch": 2.524235294117647,
      "grad_norm": 3.0570192337036133,
      "learning_rate": 2.9884564761020085e-06,
      "loss": 1.8203,
      "step": 670
    },
    {
      "epoch": 2.543058823529412,
      "grad_norm": 2.952394723892212,
      "learning_rate": 2.75857272513132e-06,
      "loss": 1.5745,
      "step": 675
    },
    {
      "epoch": 2.5618823529411765,
      "grad_norm": 3.516714572906494,
      "learning_rate": 2.5373716586730045e-06,
      "loss": 1.539,
      "step": 680
    },
    {
      "epoch": 2.580705882352941,
      "grad_norm": 2.7396414279937744,
      "learning_rate": 2.3249396299565683e-06,
      "loss": 1.5527,
      "step": 685
    },
    {
      "epoch": 2.599529411764706,
      "grad_norm": 2.5191314220428467,
      "learning_rate": 2.1213595689245386e-06,
      "loss": 1.6538,
      "step": 690
    },
    {
      "epoch": 2.6183529411764708,
      "grad_norm": 2.614373207092285,
      "learning_rate": 1.926710949857996e-06,
      "loss": 1.6903,
      "step": 695
    },
    {
      "epoch": 2.6371764705882352,
      "grad_norm": 2.747506618499756,
      "learning_rate": 1.7410697603511383e-06,
      "loss": 1.7311,
      "step": 700
    },
    {
      "epoch": 2.656,
      "grad_norm": 3.7786383628845215,
      "learning_rate": 1.5645084716469777e-06,
      "loss": 1.7871,
      "step": 705
    },
    {
      "epoch": 2.6748235294117646,
      "grad_norm": 3.110839605331421,
      "learning_rate": 1.397096010345772e-06,
      "loss": 1.6927,
      "step": 710
    },
    {
      "epoch": 2.6936470588235295,
      "grad_norm": 3.063983201980591,
      "learning_rate": 1.2388977314972238e-06,
      "loss": 1.6636,
      "step": 715
    },
    {
      "epoch": 2.712470588235294,
      "grad_norm": 3.4591963291168213,
      "learning_rate": 1.0899753930869394e-06,
      "loss": 1.6511,
      "step": 720
    },
    {
      "epoch": 2.731294117647059,
      "grad_norm": 2.713848829269409,
      "learning_rate": 9.503871319271551e-07,
      "loss": 1.5396,
      "step": 725
    },
    {
      "epoch": 2.7501176470588238,
      "grad_norm": 2.6066701412200928,
      "learning_rate": 8.201874409610733e-07,
      "loss": 1.5506,
      "step": 730
    },
    {
      "epoch": 2.7689411764705882,
      "grad_norm": 2.5422675609588623,
      "learning_rate": 6.994271479897314e-07,
      "loss": 1.6512,
      "step": 735
    },
    {
      "epoch": 2.7877647058823527,
      "grad_norm": 2.722569465637207,
      "learning_rate": 5.881533958296631e-07,
      "loss": 1.5012,
      "step": 740
    },
    {
      "epoch": 2.8065882352941176,
      "grad_norm": 3.2920100688934326,
      "learning_rate": 4.864096239091287e-07,
      "loss": 1.6716,
      "step": 745
    },
    {
      "epoch": 2.8254117647058825,
      "grad_norm": 3.076730489730835,
      "learning_rate": 3.9423555131007925e-07,
      "loss": 1.545,
      "step": 750
    },
    {
      "epoch": 2.844235294117647,
      "grad_norm": 3.1206979751586914,
      "learning_rate": 3.1166716126249663e-07,
      "loss": 1.772,
      "step": 755
    },
    {
      "epoch": 2.863058823529412,
      "grad_norm": 3.144470691680908,
      "learning_rate": 2.387366870971103e-07,
      "loss": 1.7754,
      "step": 760
    },
    {
      "epoch": 2.8818823529411763,
      "grad_norm": 3.1693906784057617,
      "learning_rate": 1.7547259966207708e-07,
      "loss": 1.7166,
      "step": 765
    },
    {
      "epoch": 2.9007058823529412,
      "grad_norm": 2.951874256134033,
      "learning_rate": 1.2189959620839686e-07,
      "loss": 1.9639,
      "step": 770
    },
    {
      "epoch": 2.9195294117647057,
      "grad_norm": 3.3032729625701904,
      "learning_rate": 7.803859074854425e-08,
      "loss": 1.6908,
      "step": 775
    },
    {
      "epoch": 2.9383529411764706,
      "grad_norm": 3.200718402862549,
      "learning_rate": 4.390670589196622e-08,
      "loss": 1.6639,
      "step": 780
    },
    {
      "epoch": 2.9571764705882355,
      "grad_norm": 2.4931979179382324,
      "learning_rate": 1.9517266160704038e-08,
      "loss": 1.6658,
      "step": 785
    },
    {
      "epoch": 2.976,
      "grad_norm": 2.879776954650879,
      "learning_rate": 4.87979278772921e-09,
      "loss": 1.7916,
      "step": 790
    },
    {
      "epoch": 2.9948235294117644,
      "grad_norm": 3.059532403945923,
      "learning_rate": 0.0,
      "loss": 1.6652,
      "step": 795
    },
    {
      "epoch": 2.9948235294117644,
      "step": 795,
      "total_flos": 2.535732853866496e+17,
      "train_loss": 1.811463862844983,
      "train_runtime": 2285.1458,
      "train_samples_per_second": 11.158,
      "train_steps_per_second": 0.348
    }
  ],
  "logging_steps": 5,
  "max_steps": 795,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.535732853866496e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
