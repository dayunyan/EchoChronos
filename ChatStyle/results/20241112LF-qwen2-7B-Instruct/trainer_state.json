{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9984317825405125,
  "eval_steps": 100,
  "global_step": 717,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.020909566126502875,
      "grad_norm": 1.4743258953094482,
      "learning_rate": 4.9994000789710415e-05,
      "loss": 2.7742,
      "step": 5
    },
    {
      "epoch": 0.04181913225300575,
      "grad_norm": 1.0107614994049072,
      "learning_rate": 4.997600603808359e-05,
      "loss": 2.4814,
      "step": 10
    },
    {
      "epoch": 0.06272869837950862,
      "grad_norm": 0.8038622736930847,
      "learning_rate": 4.994602438146344e-05,
      "loss": 2.1727,
      "step": 15
    },
    {
      "epoch": 0.0836382645060115,
      "grad_norm": 0.7005155682563782,
      "learning_rate": 4.9904070209151015e-05,
      "loss": 2.3167,
      "step": 20
    },
    {
      "epoch": 0.10454783063251437,
      "grad_norm": 0.7758643627166748,
      "learning_rate": 4.985016365649848e-05,
      "loss": 1.8511,
      "step": 25
    },
    {
      "epoch": 0.12545739675901724,
      "grad_norm": 0.7643054723739624,
      "learning_rate": 4.978433059524548e-05,
      "loss": 1.8787,
      "step": 30
    },
    {
      "epoch": 0.14636696288552012,
      "grad_norm": 0.6963573098182678,
      "learning_rate": 4.970660262110227e-05,
      "loss": 2.0177,
      "step": 35
    },
    {
      "epoch": 0.167276529012023,
      "grad_norm": 0.6832789182662964,
      "learning_rate": 4.961701703858584e-05,
      "loss": 1.9143,
      "step": 40
    },
    {
      "epoch": 0.18818609513852588,
      "grad_norm": 0.6781542301177979,
      "learning_rate": 4.951561684311608e-05,
      "loss": 2.0008,
      "step": 45
    },
    {
      "epoch": 0.20909566126502874,
      "grad_norm": 0.7186768651008606,
      "learning_rate": 4.940245070038064e-05,
      "loss": 2.1837,
      "step": 50
    },
    {
      "epoch": 0.23000522739153162,
      "grad_norm": 0.621528685092926,
      "learning_rate": 4.9277572922978586e-05,
      "loss": 2.0329,
      "step": 55
    },
    {
      "epoch": 0.2509147935180345,
      "grad_norm": 0.7184491753578186,
      "learning_rate": 4.9141043444353674e-05,
      "loss": 1.9935,
      "step": 60
    },
    {
      "epoch": 0.2718243596445374,
      "grad_norm": 0.5523587465286255,
      "learning_rate": 4.899292779003014e-05,
      "loss": 1.8031,
      "step": 65
    },
    {
      "epoch": 0.29273392577104024,
      "grad_norm": 0.7363941073417664,
      "learning_rate": 4.8833297046164594e-05,
      "loss": 2.1254,
      "step": 70
    },
    {
      "epoch": 0.31364349189754315,
      "grad_norm": 0.648826539516449,
      "learning_rate": 4.866222782542912e-05,
      "loss": 2.0326,
      "step": 75
    },
    {
      "epoch": 0.334553058024046,
      "grad_norm": 0.67071133852005,
      "learning_rate": 4.847980223024205e-05,
      "loss": 2.0329,
      "step": 80
    },
    {
      "epoch": 0.35546262415054886,
      "grad_norm": 0.8032104969024658,
      "learning_rate": 4.8286107813364015e-05,
      "loss": 1.7999,
      "step": 85
    },
    {
      "epoch": 0.37637219027705177,
      "grad_norm": 0.8578467965126038,
      "learning_rate": 4.8081237535878116e-05,
      "loss": 2.0244,
      "step": 90
    },
    {
      "epoch": 0.3972817564035546,
      "grad_norm": 0.6889342665672302,
      "learning_rate": 4.786528972257449e-05,
      "loss": 1.8325,
      "step": 95
    },
    {
      "epoch": 0.4181913225300575,
      "grad_norm": 0.6534255146980286,
      "learning_rate": 4.763836801476061e-05,
      "loss": 1.9615,
      "step": 100
    },
    {
      "epoch": 0.4181913225300575,
      "eval_loss": 1.9039404392242432,
      "eval_runtime": 43.8158,
      "eval_samples_per_second": 19.399,
      "eval_steps_per_second": 4.861,
      "step": 100
    },
    {
      "epoch": 0.4391008886565604,
      "grad_norm": 0.8755456805229187,
      "learning_rate": 4.7400581320520055e-05,
      "loss": 1.8505,
      "step": 105
    },
    {
      "epoch": 0.46001045478306324,
      "grad_norm": 0.7672187685966492,
      "learning_rate": 4.715204376244343e-05,
      "loss": 1.9749,
      "step": 110
    },
    {
      "epoch": 0.48092002090956615,
      "grad_norm": 0.8913364410400391,
      "learning_rate": 4.689287462285681e-05,
      "loss": 1.7644,
      "step": 115
    },
    {
      "epoch": 0.501829587036069,
      "grad_norm": 0.9552173018455505,
      "learning_rate": 4.662319828657371e-05,
      "loss": 1.9852,
      "step": 120
    },
    {
      "epoch": 0.5227391531625719,
      "grad_norm": 0.7639087438583374,
      "learning_rate": 4.634314418119823e-05,
      "loss": 1.9555,
      "step": 125
    },
    {
      "epoch": 0.5436487192890748,
      "grad_norm": 0.7854989767074585,
      "learning_rate": 4.605284671500805e-05,
      "loss": 1.9353,
      "step": 130
    },
    {
      "epoch": 0.5645582854155776,
      "grad_norm": 0.745197594165802,
      "learning_rate": 4.5752445212446836e-05,
      "loss": 1.8792,
      "step": 135
    },
    {
      "epoch": 0.5854678515420805,
      "grad_norm": 0.9025305509567261,
      "learning_rate": 4.544208384725742e-05,
      "loss": 1.9796,
      "step": 140
    },
    {
      "epoch": 0.6063774176685833,
      "grad_norm": 0.8474177122116089,
      "learning_rate": 4.5121911573287446e-05,
      "loss": 1.8219,
      "step": 145
    },
    {
      "epoch": 0.6272869837950863,
      "grad_norm": 0.7602227926254272,
      "learning_rate": 4.479208205300094e-05,
      "loss": 2.009,
      "step": 150
    },
    {
      "epoch": 0.6481965499215891,
      "grad_norm": 0.797386884689331,
      "learning_rate": 4.445275358373006e-05,
      "loss": 2.0745,
      "step": 155
    },
    {
      "epoch": 0.669106116048092,
      "grad_norm": 0.7077807188034058,
      "learning_rate": 4.410408902170235e-05,
      "loss": 2.1268,
      "step": 160
    },
    {
      "epoch": 0.6900156821745949,
      "grad_norm": 1.061487078666687,
      "learning_rate": 4.374625570388008e-05,
      "loss": 1.879,
      "step": 165
    },
    {
      "epoch": 0.7109252483010977,
      "grad_norm": 0.9081875085830688,
      "learning_rate": 4.337942536764901e-05,
      "loss": 1.7571,
      "step": 170
    },
    {
      "epoch": 0.7318348144276007,
      "grad_norm": 0.879791259765625,
      "learning_rate": 4.3003774068395355e-05,
      "loss": 1.8791,
      "step": 175
    },
    {
      "epoch": 0.7527443805541035,
      "grad_norm": 0.7399460077285767,
      "learning_rate": 4.26194820950103e-05,
      "loss": 1.964,
      "step": 180
    },
    {
      "epoch": 0.7736539466806064,
      "grad_norm": 0.8321090936660767,
      "learning_rate": 4.222673388336272e-05,
      "loss": 1.9864,
      "step": 185
    },
    {
      "epoch": 0.7945635128071092,
      "grad_norm": 0.7471945881843567,
      "learning_rate": 4.182571792778163e-05,
      "loss": 1.8831,
      "step": 190
    },
    {
      "epoch": 0.8154730789336121,
      "grad_norm": 0.8296023607254028,
      "learning_rate": 4.141662669059076e-05,
      "loss": 1.7982,
      "step": 195
    },
    {
      "epoch": 0.836382645060115,
      "grad_norm": 0.8818992972373962,
      "learning_rate": 4.0999656509738904e-05,
      "loss": 1.856,
      "step": 200
    },
    {
      "epoch": 0.836382645060115,
      "eval_loss": 1.8870997428894043,
      "eval_runtime": 43.8376,
      "eval_samples_per_second": 19.39,
      "eval_steps_per_second": 4.859,
      "step": 200
    },
    {
      "epoch": 0.8572922111866179,
      "grad_norm": 0.9536499977111816,
      "learning_rate": 4.0575007504569994e-05,
      "loss": 1.9272,
      "step": 205
    },
    {
      "epoch": 0.8782017773131208,
      "grad_norm": 0.802596926689148,
      "learning_rate": 4.0142883479778555e-05,
      "loss": 1.9263,
      "step": 210
    },
    {
      "epoch": 0.8991113434396236,
      "grad_norm": 0.785929262638092,
      "learning_rate": 3.970349182759623e-05,
      "loss": 1.7937,
      "step": 215
    },
    {
      "epoch": 0.9200209095661265,
      "grad_norm": 0.8581550717353821,
      "learning_rate": 3.925704342825671e-05,
      "loss": 1.9902,
      "step": 220
    },
    {
      "epoch": 0.9409304756926293,
      "grad_norm": 0.9918459057807922,
      "learning_rate": 3.880375254878649e-05,
      "loss": 2.0351,
      "step": 225
    },
    {
      "epoch": 0.9618400418191323,
      "grad_norm": 0.7597730159759521,
      "learning_rate": 3.8343836740170216e-05,
      "loss": 2.0536,
      "step": 230
    },
    {
      "epoch": 0.9827496079456352,
      "grad_norm": 1.0028953552246094,
      "learning_rate": 3.787751673294001e-05,
      "loss": 1.9151,
      "step": 235
    },
    {
      "epoch": 1.003659174072138,
      "grad_norm": 2.1859071254730225,
      "learning_rate": 3.740501633123872e-05,
      "loss": 2.1067,
      "step": 240
    },
    {
      "epoch": 1.024568740198641,
      "grad_norm": 0.8072537779808044,
      "learning_rate": 3.692656230540808e-05,
      "loss": 1.8352,
      "step": 245
    },
    {
      "epoch": 1.0454783063251438,
      "grad_norm": 0.8369348049163818,
      "learning_rate": 3.644238428315328e-05,
      "loss": 1.7356,
      "step": 250
    },
    {
      "epoch": 1.0663878724516467,
      "grad_norm": 0.9828656911849976,
      "learning_rate": 3.595271463933617e-05,
      "loss": 1.9555,
      "step": 255
    },
    {
      "epoch": 1.0872974385781495,
      "grad_norm": 0.9612281322479248,
      "learning_rate": 3.5457788384450005e-05,
      "loss": 1.6865,
      "step": 260
    },
    {
      "epoch": 1.1082070047046524,
      "grad_norm": 1.26370370388031,
      "learning_rate": 3.495784305182925e-05,
      "loss": 1.9295,
      "step": 265
    },
    {
      "epoch": 1.1291165708311552,
      "grad_norm": 1.0599355697631836,
      "learning_rate": 3.445311858364862e-05,
      "loss": 1.9116,
      "step": 270
    },
    {
      "epoch": 1.150026136957658,
      "grad_norm": 1.0477559566497803,
      "learning_rate": 3.394385721576592e-05,
      "loss": 2.1325,
      "step": 275
    },
    {
      "epoch": 1.170935703084161,
      "grad_norm": 1.1254024505615234,
      "learning_rate": 3.3430303361464235e-05,
      "loss": 1.7524,
      "step": 280
    },
    {
      "epoch": 1.1918452692106638,
      "grad_norm": 1.4871301651000977,
      "learning_rate": 3.291270349414891e-05,
      "loss": 1.8879,
      "step": 285
    },
    {
      "epoch": 1.2127548353371667,
      "grad_norm": 1.3704928159713745,
      "learning_rate": 3.239130602905594e-05,
      "loss": 1.851,
      "step": 290
    },
    {
      "epoch": 1.2336644014636695,
      "grad_norm": 1.273259162902832,
      "learning_rate": 3.186636120402834e-05,
      "loss": 1.6754,
      "step": 295
    },
    {
      "epoch": 1.2545739675901726,
      "grad_norm": 1.1111820936203003,
      "learning_rate": 3.133812095941775e-05,
      "loss": 1.9986,
      "step": 300
    },
    {
      "epoch": 1.2545739675901726,
      "eval_loss": 1.8831515312194824,
      "eval_runtime": 43.7699,
      "eval_samples_per_second": 19.42,
      "eval_steps_per_second": 4.866,
      "step": 300
    },
    {
      "epoch": 1.2754835337166754,
      "grad_norm": 1.446394443511963,
      "learning_rate": 3.080683881716906e-05,
      "loss": 2.01,
      "step": 305
    },
    {
      "epoch": 1.2963930998431783,
      "grad_norm": 1.2694956064224243,
      "learning_rate": 3.0272769759145813e-05,
      "loss": 1.9378,
      "step": 310
    },
    {
      "epoch": 1.3173026659696812,
      "grad_norm": 1.3635042905807495,
      "learning_rate": 2.9736170104755075e-05,
      "loss": 1.7594,
      "step": 315
    },
    {
      "epoch": 1.338212232096184,
      "grad_norm": 1.244959831237793,
      "learning_rate": 2.919729738793028e-05,
      "loss": 1.9149,
      "step": 320
    },
    {
      "epoch": 1.3591217982226869,
      "grad_norm": 1.4785175323486328,
      "learning_rate": 2.8656410233531232e-05,
      "loss": 1.9971,
      "step": 325
    },
    {
      "epoch": 1.3800313643491897,
      "grad_norm": 1.5215717554092407,
      "learning_rate": 2.811376823322051e-05,
      "loss": 2.0194,
      "step": 330
    },
    {
      "epoch": 1.4009409304756926,
      "grad_norm": 1.5932927131652832,
      "learning_rate": 2.7569631820875858e-05,
      "loss": 1.8603,
      "step": 335
    },
    {
      "epoch": 1.4218504966021954,
      "grad_norm": 1.4857434034347534,
      "learning_rate": 2.702426214759839e-05,
      "loss": 1.7998,
      "step": 340
    },
    {
      "epoch": 1.4427600627286985,
      "grad_norm": 1.518555998802185,
      "learning_rate": 2.647792095637654e-05,
      "loss": 1.8563,
      "step": 345
    },
    {
      "epoch": 1.4636696288552011,
      "grad_norm": 1.4381424188613892,
      "learning_rate": 2.5930870456466e-05,
      "loss": 1.8815,
      "step": 350
    },
    {
      "epoch": 1.4845791949817042,
      "grad_norm": 1.2970030307769775,
      "learning_rate": 2.53833731975458e-05,
      "loss": 1.9732,
      "step": 355
    },
    {
      "epoch": 1.5054887611082068,
      "grad_norm": 1.4737697839736938,
      "learning_rate": 2.483569194371109e-05,
      "loss": 1.9076,
      "step": 360
    },
    {
      "epoch": 1.52639832723471,
      "grad_norm": 1.4770135879516602,
      "learning_rate": 2.4288089547362925e-05,
      "loss": 1.8539,
      "step": 365
    },
    {
      "epoch": 1.5473078933612128,
      "grad_norm": 1.634273648262024,
      "learning_rate": 2.3740828823055784e-05,
      "loss": 1.9543,
      "step": 370
    },
    {
      "epoch": 1.5682174594877156,
      "grad_norm": 1.642296314239502,
      "learning_rate": 2.3194172421363132e-05,
      "loss": 1.7833,
      "step": 375
    },
    {
      "epoch": 1.5891270256142185,
      "grad_norm": 1.664088249206543,
      "learning_rate": 2.264838270282175e-05,
      "loss": 1.7774,
      "step": 380
    },
    {
      "epoch": 1.6100365917407213,
      "grad_norm": 1.591605544090271,
      "learning_rate": 2.2103721612015285e-05,
      "loss": 1.6451,
      "step": 385
    },
    {
      "epoch": 1.6309461578672244,
      "grad_norm": 1.6098212003707886,
      "learning_rate": 2.1560450551857365e-05,
      "loss": 1.6905,
      "step": 390
    },
    {
      "epoch": 1.651855723993727,
      "grad_norm": 1.6466044187545776,
      "learning_rate": 2.1018830258134695e-05,
      "loss": 1.9568,
      "step": 395
    },
    {
      "epoch": 1.6727652901202301,
      "grad_norm": 1.6004047393798828,
      "learning_rate": 2.0479120674370434e-05,
      "loss": 1.8464,
      "step": 400
    },
    {
      "epoch": 1.6727652901202301,
      "eval_loss": 1.8842506408691406,
      "eval_runtime": 43.8244,
      "eval_samples_per_second": 19.396,
      "eval_steps_per_second": 4.86,
      "step": 400
    },
    {
      "epoch": 1.6936748562467328,
      "grad_norm": 1.5791327953338623,
      "learning_rate": 1.994158082706764e-05,
      "loss": 1.7469,
      "step": 405
    },
    {
      "epoch": 1.7145844223732358,
      "grad_norm": 2.062500238418579,
      "learning_rate": 1.9406468701392937e-05,
      "loss": 1.6867,
      "step": 410
    },
    {
      "epoch": 1.7354939884997387,
      "grad_norm": 1.4037888050079346,
      "learning_rate": 1.887404111735998e-05,
      "loss": 1.8367,
      "step": 415
    },
    {
      "epoch": 1.7564035546262415,
      "grad_norm": 1.755588173866272,
      "learning_rate": 1.834455360657201e-05,
      "loss": 1.805,
      "step": 420
    },
    {
      "epoch": 1.7773131207527444,
      "grad_norm": 1.7350308895111084,
      "learning_rate": 1.7818260289582854e-05,
      "loss": 1.5107,
      "step": 425
    },
    {
      "epoch": 1.7982226868792472,
      "grad_norm": 1.7677052021026611,
      "learning_rate": 1.729541375393513e-05,
      "loss": 1.9629,
      "step": 430
    },
    {
      "epoch": 1.81913225300575,
      "grad_norm": 1.9674105644226074,
      "learning_rate": 1.677626493293416e-05,
      "loss": 1.988,
      "step": 435
    },
    {
      "epoch": 1.840041819132253,
      "grad_norm": 1.6879488229751587,
      "learning_rate": 1.6261062985215837e-05,
      "loss": 1.7671,
      "step": 440
    },
    {
      "epoch": 1.860951385258756,
      "grad_norm": 1.5445386171340942,
      "learning_rate": 1.5750055175166245e-05,
      "loss": 1.8786,
      "step": 445
    },
    {
      "epoch": 1.8818609513852587,
      "grad_norm": 1.9268525838851929,
      "learning_rate": 1.5243486754250363e-05,
      "loss": 1.8284,
      "step": 450
    },
    {
      "epoch": 1.9027705175117617,
      "grad_norm": 1.770262598991394,
      "learning_rate": 1.4741600843306813e-05,
      "loss": 1.8528,
      "step": 455
    },
    {
      "epoch": 1.9236800836382644,
      "grad_norm": 1.9162507057189941,
      "learning_rate": 1.4244638315865316e-05,
      "loss": 1.6944,
      "step": 460
    },
    {
      "epoch": 1.9445896497647674,
      "grad_norm": 1.8298656940460205,
      "learning_rate": 1.375283768254252e-05,
      "loss": 1.8617,
      "step": 465
    },
    {
      "epoch": 1.9654992158912703,
      "grad_norm": 1.6923222541809082,
      "learning_rate": 1.326643497657199e-05,
      "loss": 2.0288,
      "step": 470
    },
    {
      "epoch": 1.9864087820177732,
      "grad_norm": 1.719780445098877,
      "learning_rate": 1.2785663640523244e-05,
      "loss": 1.8341,
      "step": 475
    },
    {
      "epoch": 2.007318348144276,
      "grad_norm": 1.8481405973434448,
      "learning_rate": 1.231075441426395e-05,
      "loss": 2.2024,
      "step": 480
    },
    {
      "epoch": 2.028227914270779,
      "grad_norm": 1.4951201677322388,
      "learning_rate": 1.1841935224219452e-05,
      "loss": 1.7519,
      "step": 485
    },
    {
      "epoch": 2.049137480397282,
      "grad_norm": 1.701505422592163,
      "learning_rate": 1.1379431073982466e-05,
      "loss": 1.743,
      "step": 490
    },
    {
      "epoch": 2.0700470465237846,
      "grad_norm": 1.5922609567642212,
      "learning_rate": 1.0923463936325568e-05,
      "loss": 2.0665,
      "step": 495
    },
    {
      "epoch": 2.0909566126502876,
      "grad_norm": 1.7981623411178589,
      "learning_rate": 1.0474252646668253e-05,
      "loss": 1.7402,
      "step": 500
    },
    {
      "epoch": 2.0909566126502876,
      "eval_loss": 1.890966534614563,
      "eval_runtime": 43.7763,
      "eval_samples_per_second": 19.417,
      "eval_steps_per_second": 4.866,
      "step": 500
    },
    {
      "epoch": 2.1118661787767903,
      "grad_norm": 1.9516173601150513,
      "learning_rate": 1.003201279804983e-05,
      "loss": 1.521,
      "step": 505
    },
    {
      "epoch": 2.1327757449032934,
      "grad_norm": 1.6745071411132812,
      "learning_rate": 9.596956637658313e-06,
      "loss": 1.6222,
      "step": 510
    },
    {
      "epoch": 2.153685311029796,
      "grad_norm": 1.5728886127471924,
      "learning_rate": 9.169292964965213e-06,
      "loss": 1.5637,
      "step": 515
    },
    {
      "epoch": 2.174594877156299,
      "grad_norm": 2.0829052925109863,
      "learning_rate": 8.749227031514992e-06,
      "loss": 1.8233,
      "step": 520
    },
    {
      "epoch": 2.1955044432828017,
      "grad_norm": 1.5571500062942505,
      "learning_rate": 8.33696044241728e-06,
      "loss": 1.6117,
      "step": 525
    },
    {
      "epoch": 2.2164140094093048,
      "grad_norm": 1.265760898590088,
      "learning_rate": 7.932691059589129e-06,
      "loss": 1.628,
      "step": 530
    },
    {
      "epoch": 2.2373235755358074,
      "grad_norm": 1.9834407567977905,
      "learning_rate": 7.536612906793855e-06,
      "loss": 1.8222,
      "step": 535
    },
    {
      "epoch": 2.2582331416623105,
      "grad_norm": 1.9439172744750977,
      "learning_rate": 7.148916076521828e-06,
      "loss": 1.7833,
      "step": 540
    },
    {
      "epoch": 2.2791427077888136,
      "grad_norm": 1.61245858669281,
      "learning_rate": 6.769786638758105e-06,
      "loss": 1.6938,
      "step": 545
    },
    {
      "epoch": 2.300052273915316,
      "grad_norm": 1.999082326889038,
      "learning_rate": 6.399406551680645e-06,
      "loss": 1.8433,
      "step": 550
    },
    {
      "epoch": 2.3209618400418193,
      "grad_norm": 1.995654821395874,
      "learning_rate": 6.0379535743317896e-06,
      "loss": 1.7385,
      "step": 555
    },
    {
      "epoch": 2.341871406168322,
      "grad_norm": 2.0824341773986816,
      "learning_rate": 5.685601181305206e-06,
      "loss": 1.639,
      "step": 560
    },
    {
      "epoch": 2.362780972294825,
      "grad_norm": 1.9473258256912231,
      "learning_rate": 5.3425184794890505e-06,
      "loss": 1.6205,
      "step": 565
    },
    {
      "epoch": 2.3836905384213276,
      "grad_norm": 1.733054280281067,
      "learning_rate": 5.0088701269053044e-06,
      "loss": 1.6462,
      "step": 570
    },
    {
      "epoch": 2.4046001045478307,
      "grad_norm": 1.9788111448287964,
      "learning_rate": 4.684816253684368e-06,
      "loss": 1.8836,
      "step": 575
    },
    {
      "epoch": 2.4255096706743333,
      "grad_norm": 1.996019959449768,
      "learning_rate": 4.370512385212697e-06,
      "loss": 1.8719,
      "step": 580
    },
    {
      "epoch": 2.4464192368008364,
      "grad_norm": 1.506637692451477,
      "learning_rate": 4.066109367490426e-06,
      "loss": 1.742,
      "step": 585
    },
    {
      "epoch": 2.467328802927339,
      "grad_norm": 2.0201163291931152,
      "learning_rate": 3.7717532947348365e-06,
      "loss": 1.957,
      "step": 590
    },
    {
      "epoch": 2.488238369053842,
      "grad_norm": 1.7227953672409058,
      "learning_rate": 3.487585439264382e-06,
      "loss": 1.6168,
      "step": 595
    },
    {
      "epoch": 2.509147935180345,
      "grad_norm": 1.7506688833236694,
      "learning_rate": 3.2137421836968494e-06,
      "loss": 1.6629,
      "step": 600
    },
    {
      "epoch": 2.509147935180345,
      "eval_loss": 1.894450306892395,
      "eval_runtime": 43.8281,
      "eval_samples_per_second": 19.394,
      "eval_steps_per_second": 4.86,
      "step": 600
    },
    {
      "epoch": 2.530057501306848,
      "grad_norm": 1.8974969387054443,
      "learning_rate": 2.9503549554943694e-06,
      "loss": 1.6671,
      "step": 605
    },
    {
      "epoch": 2.550967067433351,
      "grad_norm": 1.5536389350891113,
      "learning_rate": 2.6975501638865085e-06,
      "loss": 1.6783,
      "step": 610
    },
    {
      "epoch": 2.5718766335598535,
      "grad_norm": 1.6938862800598145,
      "learning_rate": 2.4554491392018347e-06,
      "loss": 1.6254,
      "step": 615
    },
    {
      "epoch": 2.5927861996863566,
      "grad_norm": 1.5894826650619507,
      "learning_rate": 2.2241680746370093e-06,
      "loss": 1.6372,
      "step": 620
    },
    {
      "epoch": 2.6136957658128592,
      "grad_norm": 1.952500343322754,
      "learning_rate": 2.0038179704914286e-06,
      "loss": 1.8035,
      "step": 625
    },
    {
      "epoch": 2.6346053319393623,
      "grad_norm": 1.6214247941970825,
      "learning_rate": 1.794504580894052e-06,
      "loss": 1.5103,
      "step": 630
    },
    {
      "epoch": 2.6555148980658654,
      "grad_norm": 1.858913779258728,
      "learning_rate": 1.5963283630481385e-06,
      "loss": 1.6547,
      "step": 635
    },
    {
      "epoch": 2.676424464192368,
      "grad_norm": 1.8670514822006226,
      "learning_rate": 1.409384429018104e-06,
      "loss": 1.566,
      "step": 640
    },
    {
      "epoch": 2.6973340303188706,
      "grad_norm": 1.8964486122131348,
      "learning_rate": 1.2337625000817616e-06,
      "loss": 1.8222,
      "step": 645
    },
    {
      "epoch": 2.7182435964453737,
      "grad_norm": 1.8363295793533325,
      "learning_rate": 1.0695468636697514e-06,
      "loss": 1.6177,
      "step": 650
    },
    {
      "epoch": 2.739153162571877,
      "grad_norm": 1.659164547920227,
      "learning_rate": 9.168163329129397e-07,
      "loss": 1.7652,
      "step": 655
    },
    {
      "epoch": 2.7600627286983794,
      "grad_norm": 2.5247650146484375,
      "learning_rate": 7.75644208817053e-07,
      "loss": 1.8015,
      "step": 660
    },
    {
      "epoch": 2.7809722948248825,
      "grad_norm": 2.0337586402893066,
      "learning_rate": 6.46098245082849e-07,
      "loss": 1.8524,
      "step": 665
    },
    {
      "epoch": 2.801881860951385,
      "grad_norm": 1.9999184608459473,
      "learning_rate": 5.28240615588621e-07,
      "loss": 1.8801,
      "step": 670
    },
    {
      "epoch": 2.822791427077888,
      "grad_norm": 2.028395175933838,
      "learning_rate": 4.2212788455065213e-07,
      "loss": 1.737,
      "step": 675
    },
    {
      "epoch": 2.843700993204391,
      "grad_norm": 1.945116639137268,
      "learning_rate": 3.2781097937596984e-07,
      "loss": 1.961,
      "step": 680
    },
    {
      "epoch": 2.864610559330894,
      "grad_norm": 2.1713619232177734,
      "learning_rate": 2.453351662204201e-07,
      "loss": 1.9921,
      "step": 685
    },
    {
      "epoch": 2.885520125457397,
      "grad_norm": 1.634236454963684,
      "learning_rate": 1.7474002826375625e-07,
      "loss": 1.7263,
      "step": 690
    },
    {
      "epoch": 2.9064296915838996,
      "grad_norm": 1.6151849031448364,
      "learning_rate": 1.1605944671221614e-07,
      "loss": 1.5724,
      "step": 695
    },
    {
      "epoch": 2.9273392577104023,
      "grad_norm": 1.6913206577301025,
      "learning_rate": 6.932158453769877e-08,
      "loss": 1.8994,
      "step": 700
    },
    {
      "epoch": 2.9273392577104023,
      "eval_loss": 1.8942667245864868,
      "eval_runtime": 43.8549,
      "eval_samples_per_second": 19.382,
      "eval_steps_per_second": 4.857,
      "step": 700
    },
    {
      "epoch": 2.9482488238369053,
      "grad_norm": 1.8719940185546875,
      "learning_rate": 3.454887296129028e-08,
      "loss": 1.7384,
      "step": 705
    },
    {
      "epoch": 2.9691583899634084,
      "grad_norm": 1.6205339431762695,
      "learning_rate": 1.1758000687719928e-08,
      "loss": 1.7864,
      "step": 710
    },
    {
      "epoch": 2.990067956089911,
      "grad_norm": 2.2256503105163574,
      "learning_rate": 9.59905895825397e-10,
      "loss": 1.9825,
      "step": 715
    },
    {
      "epoch": 2.9984317825405125,
      "step": 717,
      "total_flos": 2.2691435895586816e+17,
      "train_loss": 1.8627949532438355,
      "train_runtime": 3648.9713,
      "train_samples_per_second": 6.289,
      "train_steps_per_second": 0.196
    }
  ],
  "logging_steps": 5,
  "max_steps": 717,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.2691435895586816e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
